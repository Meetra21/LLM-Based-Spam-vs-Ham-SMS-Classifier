{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Meetra21/LLM-Based-Spam-vs-Ham-SMS-Classifier/blob/main/LLM_Based_Spam_vs_Ham_SMS_Classifier_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LLM-Based Spam vs Ham SMS Classifier**\n",
        "Azam (Meetra) Nouri\n",
        "\n",
        "This project builds a spam vs. non-spam SMS classifier by fine-tuning a pretrained LLM (GPT-2) on the public SMS Spam Collection dataset. Messages are tokenized with the model’s tokenizer and used to train a lightweight classification head on top of the LLM’s pretrained language representations. Training runs on a Colab GPU and produces a model that outputs spam/ham probabilities for new messages, demonstrating how a pretrained LLM can be adapted into a practical text-classification system."
      ],
      "metadata": {
        "id": "w2izGPPVzlrI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1) Install + check GPU**\n",
        "\n",
        "Check that the GPU is active\n",
        "Before training, we confirm that Colab is running with the L4 GPU. If CUDA is not available, training falls back to CPU and becomes significantly slower. Printing the GPU name also verifies that the correct hardware is being used."
      ],
      "metadata": {
        "id": "2A4pI_-lrr7C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PURPOSE:\n",
        "#   Install libraries for dataset loading + GPT-2 fine-tuning.\n",
        "# WHY:\n",
        "#   - datasets: easy access to spam datasets on Hugging Face\n",
        "#   - transformers: pretrained GPT-2 + Trainer\n",
        "#   - accelerate: speeds up Trainer on GPU\n",
        "!pip install -q datasets transformers accelerate torch\n",
        "\n",
        "# PURPOSE:\n",
        "#   Confirm GPU is enabled in Colab.\n",
        "import torch\n",
        "print(\"# CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"# GPU:\", torch.cuda.get_device_name(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYN1eNqrrx0d",
        "outputId": "acbc2268-e551-499e-9585-6ad0f56a1df6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# CUDA available: True\n",
            "# GPU: NVIDIA L4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2) Load an appropriate spam dataset (SMS Spam Collection)**\n",
        "\n",
        "Next, we load an appropriate labeled dataset for spam detection, such as the SMS Spam Collection. This dataset contains short SMS messages and corresponding labels indicating whether each message is spam or non-spam."
      ],
      "metadata": {
        "id": "mx5vhP11rsQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PURPOSE:\n",
        "#   Load a real spam-vs-ham dataset.\n",
        "from datasets import load_dataset\n",
        "\n",
        "# SMS Spam Collection (ham/spam), 5,574 messages\n",
        "# Source: Hugging Face mirror of the UCI dataset\n",
        "ds = load_dataset(\"ucirvine/sms_spam\")  # returns a DatasetDict (usually only \"train\")\n",
        "\n",
        "print(ds)\n",
        "print(ds[\"train\"][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k63_ZpOBr64x",
        "outputId": "2af6011f-1987-4bec-c934-e6d4ee52cc88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['sms', 'label'],\n",
            "        num_rows: 5574\n",
            "    })\n",
            "})\n",
            "{'sms': 'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\\n', 'label': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3) Prepare labels + train/validation split**\n",
        "\n",
        "We then ensure that labels are converted into a clean numeric format: 0 for ham (non-spam) and 1 for spam. After that, the dataset is split into training and validation sets. The training set is used to learn patterns, while the validation set is used to measure generalization on unseen data."
      ],
      "metadata": {
        "id": "5kqT-wdarsZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PURPOSE:\n",
        "#   Ensure labels are correctly mapped to integers:\n",
        "#     ham -> 0\n",
        "#     spam -> 1\n",
        "# WHY:\n",
        "#   If labels are already 0/1 and you treat them as strings,\n",
        "#   you can accidentally convert EVERYTHING to ham (0),\n",
        "#   which makes the model predict ham for all messages.\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "print(\"# Raw label examples:\", [ds[\"train\"][i][\"label\"] for i in range(10)])\n",
        "print(\"# Raw label counts:\", Counter(ds[\"train\"][\"label\"]))\n",
        "\n",
        "def normalize_labels(example):\n",
        "    lab = example[\"label\"]\n",
        "\n",
        "    # Case 1: already numeric 0/1\n",
        "    if isinstance(lab, (int, bool)):\n",
        "        example[\"label\"] = int(lab)\n",
        "        return example\n",
        "\n",
        "    # Case 2: numeric but stored as string \"0\"/\"1\"\n",
        "    if isinstance(lab, str) and lab.strip() in {\"0\", \"1\"}:\n",
        "        example[\"label\"] = int(lab.strip())\n",
        "        return example\n",
        "\n",
        "    # Case 3: stored as \"ham\"/\"spam\"\n",
        "    if isinstance(lab, str):\n",
        "        s = lab.strip().lower()\n",
        "        if s == \"spam\":\n",
        "            example[\"label\"] = 1\n",
        "        elif s == \"ham\":\n",
        "            example[\"label\"] = 0\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown label value: {lab}\")\n",
        "        return example\n",
        "\n",
        "    raise ValueError(f\"Unhandled label type/value: {type(lab)} {lab}\")\n",
        "\n",
        "data = ds[\"train\"].map(normalize_labels)\n",
        "\n",
        "print(\"# Normalized label counts:\", Counter(data[\"label\"]))\n",
        "\n",
        "# Train/val split (same as before)\n",
        "split = data.train_test_split(test_size=0.2, seed=42)\n",
        "train_ds = split[\"train\"]\n",
        "val_ds   = split[\"test\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "256a5fb872d140189f786170f3446bf7",
            "7fc52bcc9e1144b19a66995383f10a57",
            "cb1f081923c64c4f85badf4c3616d78c",
            "3e7e321486a94ac48fe3f58de82ef3e4",
            "8c6539b10531430885dce375e89e0e0e",
            "c431918cacc4453f851464128700899b",
            "ea4d002f338049d8825dab8711e13cb7",
            "7bc87d70201d42e9994cd2aa0cbe68da",
            "55371a2f118241468796be8ff1575a42",
            "449974f8d2c74099a9bd443016311106",
            "5f12c3a843d74af0a6c2dac39c237779"
          ]
        },
        "id": "_AMwl6lZsF3z",
        "outputId": "b3883b29-8cb0-431c-b782-c906d7d9200a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Raw label examples: [0, 0, 1, 0, 0, 1, 0, 0, 1, 1]\n",
            "# Raw label counts: Counter({0: 4827, 1: 747})\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/5574 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "256a5fb872d140189f786170f3446bf7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Normalized label counts: Counter({0: 4827, 1: 747})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4) Tokenize text with GPT-2 tokenizer**\n",
        "\n",
        "LLMs do not train directly on raw text, so we convert each message into token IDs using the model’s tokenizer. We also create an attention mask so the model can distinguish real tokens from padding. Truncation to a reasonable maximum length (for example, 64 tokens) is applied because SMS messages are short and shorter sequences train faster."
      ],
      "metadata": {
        "id": "K2AOSA3hrsdz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PURPOSE:\n",
        "#   Tokenize the SMS messages into input_ids/attention_mask for GPT-2.\n",
        "# WHY:\n",
        "#   The model can only train on token IDs (numbers), not raw strings.\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# PURPOSE:\n",
        "#   GPT-2 has no pad token by default. We set pad_token = eos_token so batching works.\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# PURPOSE:\n",
        "#   Identify which column contains the message text.\n",
        "# WHY:\n",
        "#   The dataset columns are ['sms', 'label'], so we use 'sms'.\n",
        "text_col = \"sms\"\n",
        "\n",
        "def tokenize_fn(batch):\n",
        "    # PURPOSE:\n",
        "    #   Convert a batch of SMS strings into token IDs.\n",
        "    # truncation=True prevents very long texts from exceeding model limits.\n",
        "    return tokenizer(batch[text_col], truncation=True)\n",
        "\n",
        "# PURPOSE:\n",
        "#   Apply tokenization to every row and remove the original text column afterward.\n",
        "train_tok = train_ds.map(tokenize_fn, batched=True, remove_columns=[text_col])\n",
        "val_tok   = val_ds.map(tokenize_fn, batched=True, remove_columns=[text_col])\n",
        "\n",
        "print(train_tok[0].keys())  # should include input_ids, attention_mask, label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99,
          "referenced_widgets": [
            "0f14d068de1648209b505d358da74654",
            "e2e269f6c0be48e5ad07094160f386e9",
            "95317243fcfd42faacabb10c32ed134d",
            "cc7204c480244e03bbefa1659b959931",
            "b6316388197e46d29304bf2ced52cc34",
            "5beaf8891fc941c18ae5c29414fb2758",
            "7deccbd5747e4223a3e8cc9a5fe53267",
            "1ea021821e6f49a9a3543b66ea8e2f0f",
            "5152245d40fc4c22bd888418e11442af",
            "db5f46e900234cd0966a6007298c4bbc",
            "3984ef7e089e496583d0ba8464000d71",
            "c1dd9d1bf07b48cbad19337342f74efb",
            "03f1b59bdb24427c808aa1f8ffcdcd04",
            "8a7b9a662b8943258dc3f2d6d3d6c933",
            "1fa27663327949fd9e75784ea3d7ab12",
            "5f0278b811334df4b0dc1ad2a996f0cb",
            "86234e261a1a4515a8c89441239f897a",
            "40b46c5697424aaca8ce90288e21ecb1",
            "241e0cb6a9254b429b23119469e808eb",
            "3d8b101202644ae4b140f62f665fda72",
            "1a7bba3db3e54441bb4283b3aab152c0",
            "97555f153b6e40598e7e03207f085dc8"
          ]
        },
        "id": "iA7_pSRasMjz",
        "outputId": "08323c58-af5b-42a5-d456-9731b2fe5616"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/4459 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0f14d068de1648209b505d358da74654"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1115 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c1dd9d1bf07b48cbad19337342f74efb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['label', 'input_ids', 'attention_mask'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5) Load pretrained LLM GPT-2 as a sequence classifier**\n",
        "\n",
        "We load a pretrained LLM and attach a classification head on top of it. Instead of generating text, the model outputs two scores—one for ham and one for spam. Since GPT-style models typically do not include a padding token by default, a pad token (often the EOS token) is set to ensure stable batching."
      ],
      "metadata": {
        "id": "hdnG0ZErrsh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PURPOSE:\n",
        "#   Use GPT-2 pretrained weights but change the head to classification (2 labels).\n",
        "# WHY:\n",
        "#   We’re not generating text; we’re predicting spam vs ham.\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=2\n",
        ")\n",
        "\n",
        "# PURPOSE:\n",
        "#   Set padding token id inside the model config to match tokenizer.\n",
        "# WHY:\n",
        "#   Avoids padding-related warnings and ensures attention_mask works properly.\n",
        "model.config.pad_token_id = tokenizer.pad_token_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213,
          "referenced_widgets": [
            "98ef592c1f9641d29b9879fbe029259f",
            "ec60b05ba46a46c5a7a14732a2828a01",
            "3625e0b868374b168df71a84be89b014",
            "90be172a3b6f470fb5d04db7ce982931",
            "0428465d81cf49bf9da5235e68f18414",
            "e65cc6384c904da999af6e1091bd4d7d",
            "efaeeace55b549cc9946b6fcc271c2bb",
            "2f41461c4e0d4acca446bf7ac90c927e",
            "d4c82aa7d68b4918b855409d2a936a19",
            "df18afc189cf49b09f128cb1e087e357",
            "3aac5326a38c47acbae979808b22fb75"
          ]
        },
        "id": "CtPkOFkjsV_V",
        "outputId": "3b583b93-1c71-4e84-d7fc-35a3b67661c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/148 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "98ef592c1f9641d29b9879fbe029259f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "GPT2ForSequenceClassification LOAD REPORT from: gpt2\n",
            "Key                  | Status     | \n",
            "---------------------+------------+-\n",
            "h.{0...11}.attn.bias | UNEXPECTED | \n",
            "score.weight         | MISSING    | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
            "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6) Train with Trainer**\n",
        "\n",
        "We choose training settings such as batch size, learning rate, and number of epochs. Mixed precision (FP16) is enabled on the GPU to speed up training and reduce memory usage. A data collator pads batches dynamically, and metrics (such as accuracy) are defined to track performance during evaluation."
      ],
      "metadata": {
        "id": "p6fZY97mrslr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PURPOSE: install the evaluate library used for metrics (accuracy, f1, etc.)\n",
        "!pip install -q evaluate"
      ],
      "metadata": {
        "id": "9tH9Jbiatpu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PURPOSE:\n",
        "#   Build TrainingArguments in a way that works across different transformers versions.\n",
        "# WHY:\n",
        "#   Our installed transformers doesn't recognize 'evaluation_strategy'.\n",
        "#   Some versions use different names (or don't support evaluation at all in TrainingArguments).\n",
        "#   This code inspects TrainingArguments.__init__ and only passes supported args.\n",
        "\n",
        "import inspect\n",
        "import transformers\n",
        "from transformers import TrainingArguments, Trainer, DataCollatorWithPadding\n",
        "import numpy as np\n",
        "import torch\n",
        "import evaluate\n",
        "\n",
        "print(\"# transformers version:\", transformers.__version__)\n",
        "\n",
        "# PURPOSE:\n",
        "#   Padding collator: pads each batch to the longest sequence in that batch.\n",
        "# WHY:\n",
        "#   SMS messages have different lengths; batching needs padding.\n",
        "collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# PURPOSE:\n",
        "#   Accuracy metric (evaluate library).\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    # PURPOSE: compute accuracy from logits\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    return accuracy.compute(predictions=preds, references=labels)\n",
        "\n",
        "# ----------------------------\n",
        "# Auto-detect supported args\n",
        "# ----------------------------\n",
        "sig = inspect.signature(TrainingArguments.__init__)\n",
        "supported = set(sig.parameters.keys())\n",
        "\n",
        "# We create a \"desired\" args dict (what we WANT)\n",
        "desired = dict(\n",
        "    output_dir=\"./gpt2_spam_cls\",\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=5e-5,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    logging_steps=50,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "# These keys vary across versions; we add them only if supported\n",
        "# Newer versions: evaluation_strategy/save_strategy\n",
        "# Some versions: eval_strategy/save_strategy\n",
        "if \"evaluation_strategy\" in supported:\n",
        "    desired[\"evaluation_strategy\"] = \"epoch\"\n",
        "if \"eval_strategy\" in supported:\n",
        "    desired[\"eval_strategy\"] = \"epoch\"\n",
        "\n",
        "if \"save_strategy\" in supported:\n",
        "    desired[\"save_strategy\"] = \"epoch\"\n",
        "if \"save_steps\" in supported:\n",
        "    desired[\"save_steps\"] = 200  # fallback if save_strategy not available\n",
        "\n",
        "# Filter out unsupported keys to prevent TypeError\n",
        "safe_args = {k: v for k, v in desired.items() if k in supported}\n",
        "\n",
        "print(\"# TrainingArguments keys used:\", sorted(safe_args.keys()))\n",
        "\n",
        "args = TrainingArguments(**safe_args)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_tok,\n",
        "    eval_dataset=val_tok,\n",
        "    data_collator=collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Evaluation only works if eval_dataset is supported by Trainer version\n",
        "print(trainer.evaluate())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360,
          "referenced_widgets": [
            "cbe3dd3200e04f0684b0ed823f8eef3f",
            "ac98de73935640548e5d5317a5916494",
            "674bfa045d64443aa4d8d520a7c25934",
            "8481c5cffc1c488c8687498ffd8de235",
            "4638c0af7cbb4864a7c1ce5ff783b6a9",
            "92d47a40852548578a285864f358e8bd",
            "ab8168fc064142c2ac550f8090849f42",
            "c89e808e5e154185aaae707eaedc61af",
            "9b7ac8b93ee8497eabb9e95241984f11",
            "a8de2270949941babd97ee4fc667365d",
            "ffa8e37b8cd54e31911832fa8e84f4e5",
            "8a9aa769e79943af8ad76ab39da102d0",
            "6512688c95e94d34bc4c003e0ffb12dd",
            "cf538aa71a074c74a1d3d51390106185",
            "ed30f3c2e2a84903adf9aea2e498ecdb",
            "051c49d0ff00415c8c2f0475e62ff689",
            "2ecdaa5b7cbe493a98867c55019fb258",
            "3a93475a68be4112966efd09cdd83da0",
            "b20d8fbdaeb14a7bbe9cc6a6cf03e7d2",
            "346e19396be14109a7bae2d89d018983",
            "faca5ff978764899a6b9649f270ee2ae",
            "cb463ac512994ce2b58af9d6d8d45b6e",
            "ce2914adf6714c1f958bfada0def7629",
            "91ccd1a175b24b4eb95d1d15fd7f9b7a",
            "da503d9bb2604e3b9c4d44a4541b9b58",
            "0049f08c983641c38a83aad1859c6273",
            "9dcf2f4adf9e4e15bf55dff188db64ed",
            "df0d957b77654fa5a3b1db3f6d32f5ec",
            "4583fec7e404451985581867c133e4c5",
            "973d99ce200643c49a878aebfa245fac",
            "dabe76cacb3449a485d68dc42c1f4d39",
            "fffbdc291fd144bca28b3a1037264254",
            "2ff974b922bb4ae6945675eb49da60ff"
          ]
        },
        "id": "cKe4iWKutKrj",
        "outputId": "7b56153c-e19b-4c67-acad-548b24eaf7f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# transformers version: 5.0.0\n",
            "# TrainingArguments keys used: ['eval_strategy', 'fp16', 'learning_rate', 'logging_steps', 'num_train_epochs', 'output_dir', 'per_device_eval_batch_size', 'per_device_train_batch_size', 'report_to', 'save_steps', 'save_strategy']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='837' max='837' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [837/837 01:12, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.060115</td>\n",
              "      <td>0.047266</td>\n",
              "      <td>0.992825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.012148</td>\n",
              "      <td>0.051266</td>\n",
              "      <td>0.992825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.005732</td>\n",
              "      <td>0.048732</td>\n",
              "      <td>0.992825</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cbe3dd3200e04f0684b0ed823f8eef3f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8a9aa769e79943af8ad76ab39da102d0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ce2914adf6714c1f958bfada0def7629"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='35' max='35' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [35/35 00:01]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.04873223975300789, 'eval_accuracy': 0.9928251121076234, 'eval_runtime': 1.1453, 'eval_samples_per_second': 973.552, 'eval_steps_per_second': 30.56, 'epoch': 3.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 7 — Fine-tune , evaluate and Test the classifier on new messages**\n",
        "\n",
        "Training is performed by repeatedly feeding tokenized batches into the model, computing the classification loss, and updating weights to reduce errors."
      ],
      "metadata": {
        "id": "0Bnz6auArsqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PURPOSE:\n",
        "#   Run inference: spam probability for a new message.\n",
        "# WHY:\n",
        "#   This is how we use the trained chatbot as a spam detector.\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def predict_spam(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(model.device)\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "        probs = F.softmax(logits, dim=-1)[0].detach().cpu().tolist()\n",
        "    # label 1 = spam (by our mapping)\n",
        "    return {\"ham_prob\": probs[0], \"spam_prob\": probs[1]}\n",
        "\n",
        "print(predict_spam(\"Congratulations! You won a free iPhone. Click this link now!\"))\n",
        "print(predict_spam(\"Are we still meeting at 3pm today?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65CV8bbiufRy",
        "outputId": "cb7cce09-4c44-4b67-b311-8f1a88faa284"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'ham_prob': 0.006192990578711033, 'spam_prob': 0.9938070178031921}\n",
            "{'ham_prob': 0.9974614381790161, 'spam_prob': 0.0025385187473148108}\n"
          ]
        }
      ]
    }
  ]
}